{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library and data imports##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# review = \"Please note that this is purely my experience of the game during the stupid hours I have sunk into it. I won't be completely critical of the game and I'll give credit to a couple of things, most notably the gameplay. I like the animations and the motion of the players, they are fun to control. I play on normal gameplay speed and it feels great. Players are responsive and it is very fun trying to keep shape defensively (especially against ultimate difficulty) but with practice, you get there. Attacking is satisfying too, I like the different dribbling styles, although R1 dribbling at the time of this review is a little overpowered, but even still, I am happy that it's possible to dribble past players in this year's game and score some lovely solo goals. I play purely offline, as where I am currently located I just cannot get a good enough connection to have any kind of online experience. HOWEVER - despite EA releasing updates, not one of them has combated the biggest issue myself and many MANY others face, which is the abhorrent optimisation which is leading to stutters and for some, lag and game crashes. I would not want to know how many of those hours I have spent trialling fixes and it is the first time in my gaming experience to encounter a game that just won't play. Sometimes a workaround will fix things temporarily, but upon the next launch, it just doesn't work again. This to me is unacceptable. They are aware of the problem, yet nothing has changed now for well over a month of it being released. The hell I've been through to sort it should never be required to play any game. I paid £80, but I feel like EA owe me a year's salary for the amount of work I've done on their game for them. Crooks. Stutters in these kinds of games destroy any pleasurable experience you can get from it. Imagine playing an F1 game, but before every corner it stutters for 3 seconds and when it fixes itself, you've crashed into an entire neighbouring village and caused £500 million worth of damage, you'd say 'Forget it' and toss it in the bin. The same goes for football games. You need precision and reactions because timing is important. You can't do that with 3-second stutters intermittently during a match. How can nobody have seen this? I feel like Mugatu from Zoolander. It was a new era for the franchise, a fresh start, so I allowed myself to believe that things may be different, but it just isn't the case for any of the offline modes. If you think adding cut scenes of players winning the Balon d'Or (where nobody even speaks other than some announcer, which freaks me out) and an open-top bus parade of trophies you've won (which is cool the first time, but it is the same every single time after) then you don't understand why we play football games - WE WANT TO PLAY, NOT SIT THERE AND WATCH SILENT MOVIES. Is this their attempt at making it more immersive? Disaster. True, some promising ideas were introduced, like tactical vision and hiring coaches in manager mode, but here's the thing - that was bugged when it was released. Imagine working on a new feature, but for it not to work properly. What I mean is, that you can't fire coaches, you'll lock the game and have no chance to back out. I actually hope it doesn't get patched, because it doesn't matter, it tells us what we already know - they don't care. It's a shame because I think that if they'd just put more effort into it, this could've been a really good game. The foundation is there with the gameplay, and that's the heart of any good football game. But why not just stop being a bunch of fraggles and develop your offline game modes more? The amount of work they put into 'create a team' for example, which was just uninspiring, is what I am talking about. Put that work into actually making a comprehensive, immersive career mode. 'Create a team' was fantastic in the F1 game, it works in that circumstance, but for a manager mode on a football game? It isn't what we want. For those who can't be bothered to read all that: you'll feel just like Mugatu from Zoolander - It really is the same face.\"\n",
    "# review2 = 'MWIII is the worst campaign of all time. 14 short missions and half are just spec ops missions from MWII, dropping you in a section of the warzone map and giving you objectives. It was like a warzone tutorial. The story is also far less than adequate. This seems like it should have been a DLC for MWII . I have never been more upset with a game. The multiplayer needs to be phenomenal for this game to be worth buying at all.'\n",
    "\n",
    "# reviews = [review, review2]\n",
    "df = pd.read_csv('dataset.csv')\n",
    "reviews = df['review']\n",
    "y_pre = df['voted_up']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with imbalanced data ##\n",
    "This would be where we deal with the fact that most of the reviews are positive and will probably do subsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.649185001096411\n",
      "27362\n",
      "54724\n"
     ]
    }
   ],
   "source": [
    "from random import sample\n",
    "\n",
    "up = df[df['voted_up'] == True]\n",
    "down = df[df['voted_up'] == False]\n",
    "print(len(up)/len(down))\n",
    "\n",
    "df_sub = down\n",
    "df_sub = pd.concat([df_sub, (up.sample(n=len(down)))])\n",
    "\n",
    "print(len(down))\n",
    "print(len(df_sub))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = np.array(df_sub['review'])\n",
    "y_pre = np.array(df_sub['voted_up'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization ##\n",
    "Can cover the following hyperparameters (and their tuning):\n",
    "- How do we deal with contractions?\n",
    "- What kind of normalization do we do? (convert accents to ascii? stemming? lemmatization?)\n",
    "- Do we use stopwords? Which ones?\n",
    "- Other CountVectorizer hyperparams\n",
    "- How do we handle punctuation?  \n",
    "- How much should we subsample? (Need to determine performance metric first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing so the model can understand the reviews\n",
    "pp_reviews = []\n",
    "y = []\n",
    "\n",
    "# filters out reviews with non-ascii characters -- we get an error if we don't include this. Need to narrow down the exact cause if possible.\n",
    "for i in range(0, len(reviews)-1):\n",
    "    if str(reviews[i]).isascii():\n",
    "        pp_reviews.append(str(reviews[i]))\n",
    "        y.append(y_pre[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_testing_accuracy(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    # X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n",
    "\n",
    "    sgd_classifier = SGDClassifier(random_state=42)\n",
    "    sgd_classifier.fit(X_train, y_train)\n",
    "    y_pred = sgd_classifier.predict(X_test)\n",
    "    return accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_vectorization_transformation(count_vect, X):\n",
    "    # tokenizes the reviews\n",
    "    X_train_counts = count_vect.fit_transform(np.array(X))\n",
    "\n",
    "    # transforms them so we deal with term frequencies rather than term counts\n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    X = tfidf_transformer.fit_transform(X_train_counts)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline: 0.973395026026605\n"
     ]
    }
   ],
   "source": [
    "# configuring hyperparameters for the count vectorizer\n",
    "\n",
    "# baseline\n",
    "count_vect = CountVectorizer()\n",
    "X = do_vectorization_transformation(count_vect, pp_reviews)\n",
    "print(\"baseline: \" + str(get_testing_accuracy(X, y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_df: 0.0, max_df: 0.0: invalid combination\n",
      "min_df: 0.0, max_df: 0.1: 0.9713541666666666\n",
      "min_df: 0.0, max_df: 0.2: 0.9721257716049383\n",
      "min_df: 0.0, max_df: 0.30000000000000004: 0.9754050925925926\n",
      "min_df: 0.0, max_df: 0.4: 0.9729938271604939\n",
      "min_df: 0.0, max_df: 0.5: 0.9729938271604939\n",
      "min_df: 0.0, max_df: 0.6000000000000001: 0.9734760802469136\n",
      "min_df: 0.0, max_df: 0.7000000000000001: 0.9734760802469136\n",
      "min_df: 0.0, max_df: 0.8: 0.9734760802469136\n",
      "min_df: 0.0, max_df: 0.9: 0.9734760802469136\n",
      "min_df: 0.0, max_df: 1.0: 0.9734760802469136\n",
      "min_df: 0.1, max_df: 0.0: invalid combination\n",
      "min_df: 0.1, max_df: 0.1: invalid combination\n",
      "min_df: 0.1, max_df: 0.2: 0.6706211419753086\n",
      "min_df: 0.1, max_df: 0.30000000000000004: 0.6829668209876543\n",
      "min_df: 0.1, max_df: 0.4: 0.6921296296296297\n",
      "min_df: 0.1, max_df: 0.5: 0.6990740740740741\n",
      "min_df: 0.1, max_df: 0.6000000000000001: 0.7091049382716049\n",
      "min_df: 0.1, max_df: 0.7000000000000001: 0.7091049382716049\n",
      "min_df: 0.1, max_df: 0.8: 0.7091049382716049\n",
      "min_df: 0.1, max_df: 0.9: 0.7091049382716049\n",
      "min_df: 0.1, max_df: 1.0: 0.7091049382716049\n",
      "min_df: 0.2, max_df: 0.0: invalid combination\n",
      "min_df: 0.2, max_df: 0.1: invalid combination\n",
      "min_df: 0.2, max_df: 0.2: invalid combination\n",
      "min_df: 0.2, max_df: 0.30000000000000004: 0.6537422839506173\n",
      "min_df: 0.2, max_df: 0.4: 0.646508487654321\n",
      "min_df: 0.2, max_df: 0.5: 0.6576967592592593\n",
      "min_df: 0.2, max_df: 0.6000000000000001: 0.6548996913580247\n",
      "min_df: 0.2, max_df: 0.7000000000000001: 0.6548996913580247\n",
      "min_df: 0.2, max_df: 0.8: 0.6548996913580247\n",
      "min_df: 0.2, max_df: 0.9: 0.6548996913580247\n",
      "min_df: 0.2, max_df: 1.0: 0.6548996913580247\n",
      "min_df: 0.30000000000000004, max_df: 0.0: invalid combination\n",
      "min_df: 0.30000000000000004, max_df: 0.1: invalid combination\n",
      "min_df: 0.30000000000000004, max_df: 0.2: invalid combination\n",
      "min_df: 0.30000000000000004, max_df: 0.30000000000000004: invalid combination\n",
      "min_df: 0.30000000000000004, max_df: 0.4: 0.6208526234567902\n",
      "min_df: 0.30000000000000004, max_df: 0.5: 0.6382137345679012\n",
      "min_df: 0.30000000000000004, max_df: 0.6000000000000001: 0.6375385802469136\n",
      "min_df: 0.30000000000000004, max_df: 0.7000000000000001: 0.6375385802469136\n",
      "min_df: 0.30000000000000004, max_df: 0.8: 0.6375385802469136\n",
      "min_df: 0.30000000000000004, max_df: 0.9: 0.6375385802469136\n",
      "min_df: 0.30000000000000004, max_df: 1.0: 0.6375385802469136\n",
      "min_df: 0.4, max_df: 0.0: invalid combination\n",
      "min_df: 0.4, max_df: 0.1: invalid combination\n",
      "min_df: 0.4, max_df: 0.2: invalid combination\n",
      "min_df: 0.4, max_df: 0.30000000000000004: invalid combination\n",
      "min_df: 0.4, max_df: 0.4: invalid combination\n",
      "min_df: 0.4, max_df: 0.5: 0.6315586419753086\n",
      "min_df: 0.4, max_df: 0.6000000000000001: 0.6286651234567902\n",
      "min_df: 0.4, max_df: 0.7000000000000001: 0.6286651234567902\n",
      "min_df: 0.4, max_df: 0.8: 0.6286651234567902\n",
      "min_df: 0.4, max_df: 0.9: 0.6286651234567902\n",
      "min_df: 0.4, max_df: 1.0: 0.6286651234567902\n",
      "min_df: 0.5, max_df: 0.0: invalid combination\n",
      "min_df: 0.5, max_df: 0.1: invalid combination\n",
      "min_df: 0.5, max_df: 0.2: invalid combination\n",
      "min_df: 0.5, max_df: 0.30000000000000004: invalid combination\n",
      "min_df: 0.5, max_df: 0.4: invalid combination\n",
      "min_df: 0.5, max_df: 0.5: invalid combination\n",
      "min_df: 0.5, max_df: 0.6000000000000001: 0.6358024691358025\n",
      "min_df: 0.5, max_df: 0.7000000000000001: 0.6358024691358025\n",
      "min_df: 0.5, max_df: 0.8: 0.6358024691358025\n",
      "min_df: 0.5, max_df: 0.9: 0.6358024691358025\n",
      "min_df: 0.5, max_df: 1.0: 0.6358024691358025\n",
      "min_df: 0.6000000000000001, max_df: 0.0: invalid combination\n",
      "min_df: 0.6000000000000001, max_df: 0.1: invalid combination\n",
      "min_df: 0.6000000000000001, max_df: 0.2: invalid combination\n",
      "min_df: 0.6000000000000001, max_df: 0.30000000000000004: invalid combination\n",
      "min_df: 0.6000000000000001, max_df: 0.4: invalid combination\n",
      "min_df: 0.6000000000000001, max_df: 0.5: invalid combination\n",
      "min_df: 0.6000000000000001, max_df: 0.6000000000000001: invalid combination\n",
      "min_df: 0.6000000000000001, max_df: 0.7000000000000001: invalid combination\n",
      "min_df: 0.6000000000000001, max_df: 0.8: invalid combination\n",
      "min_df: 0.6000000000000001, max_df: 0.9: invalid combination\n",
      "min_df: 0.6000000000000001, max_df: 1.0: invalid combination\n",
      "min_df: 0.7000000000000001, max_df: 0.0: invalid combination\n",
      "min_df: 0.7000000000000001, max_df: 0.1: invalid combination\n",
      "min_df: 0.7000000000000001, max_df: 0.2: invalid combination\n",
      "min_df: 0.7000000000000001, max_df: 0.30000000000000004: invalid combination\n",
      "min_df: 0.7000000000000001, max_df: 0.4: invalid combination\n",
      "min_df: 0.7000000000000001, max_df: 0.5: invalid combination\n",
      "min_df: 0.7000000000000001, max_df: 0.6000000000000001: invalid combination\n",
      "min_df: 0.7000000000000001, max_df: 0.7000000000000001: invalid combination\n",
      "min_df: 0.7000000000000001, max_df: 0.8: invalid combination\n",
      "min_df: 0.7000000000000001, max_df: 0.9: invalid combination\n",
      "min_df: 0.7000000000000001, max_df: 1.0: invalid combination\n",
      "min_df: 0.8, max_df: 0.0: invalid combination\n",
      "min_df: 0.8, max_df: 0.1: invalid combination\n",
      "min_df: 0.8, max_df: 0.2: invalid combination\n",
      "min_df: 0.8, max_df: 0.30000000000000004: invalid combination\n",
      "min_df: 0.8, max_df: 0.4: invalid combination\n",
      "min_df: 0.8, max_df: 0.5: invalid combination\n",
      "min_df: 0.8, max_df: 0.6000000000000001: invalid combination\n",
      "min_df: 0.8, max_df: 0.7000000000000001: invalid combination\n",
      "min_df: 0.8, max_df: 0.8: invalid combination\n",
      "min_df: 0.8, max_df: 0.9: invalid combination\n",
      "min_df: 0.8, max_df: 1.0: invalid combination\n",
      "min_df: 0.9, max_df: 0.0: invalid combination\n",
      "min_df: 0.9, max_df: 0.1: invalid combination\n",
      "min_df: 0.9, max_df: 0.2: invalid combination\n",
      "min_df: 0.9, max_df: 0.30000000000000004: invalid combination\n",
      "min_df: 0.9, max_df: 0.4: invalid combination\n",
      "min_df: 0.9, max_df: 0.5: invalid combination\n",
      "min_df: 0.9, max_df: 0.6000000000000001: invalid combination\n",
      "min_df: 0.9, max_df: 0.7000000000000001: invalid combination\n",
      "min_df: 0.9, max_df: 0.8: invalid combination\n",
      "min_df: 0.9, max_df: 0.9: invalid combination\n",
      "min_df: 0.9, max_df: 1.0: invalid combination\n",
      "min_df: 1.0, max_df: 0.0: invalid combination\n",
      "min_df: 1.0, max_df: 0.1: invalid combination\n",
      "min_df: 1.0, max_df: 0.2: invalid combination\n",
      "min_df: 1.0, max_df: 0.30000000000000004: invalid combination\n",
      "min_df: 1.0, max_df: 0.4: invalid combination\n",
      "min_df: 1.0, max_df: 0.5: invalid combination\n",
      "min_df: 1.0, max_df: 0.6000000000000001: invalid combination\n",
      "min_df: 1.0, max_df: 0.7000000000000001: invalid combination\n",
      "min_df: 1.0, max_df: 0.8: invalid combination\n",
      "min_df: 1.0, max_df: 0.9: invalid combination\n",
      "min_df: 1.0, max_df: 1.0: invalid combination\n"
     ]
    }
   ],
   "source": [
    "# max_df/min_df\n",
    "for min_df in np.arange(0, 1.1, 0.1):\n",
    "    for max_df in np.arange(0, 1.1, 0.1):\n",
    "        try:\n",
    "            count_vect = CountVectorizer(min_df=min_df, max_df=max_df)\n",
    "            X = do_vectorization_transformation(count_vect, pp_reviews)\n",
    "            print(\"min_df: \" + str(min_df) + \", max_df: \" + str(max_df) + \": \" + str(get_testing_accuracy(X, y)))\n",
    "        except:\n",
    "            print(\"min_df: \" + str(min_df) + \", max_df: \" + str(max_df) + \": \" + \"invalid combination\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all other factors equal, the best min_df/max_df combination was 0.0, 0.1 with a testing accuracy of 0.9803 when using an SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with stopwords: 0.9703896604938271\n"
     ]
    }
   ],
   "source": [
    "# with/without stopwords\n",
    "# taken from NLTK stopword set, stripped of punctuation\n",
    "stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'youre', 'youve', 'youll', 'youd', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'shes', 'her', 'hers', 'herself', 'it', 'its', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'thatll', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'dont', 'should', 'shouldve', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'arent', 'couldn', 'couldnt', 'didn', 'didnt', 'doesn', 'doesnt', 'hadn', 'hadnt', 'hasn', 'hasnt', 'haven', 'havent', 'isn', 'isnt', 'ma', 'mightn', 'mightnt', 'mustn', 'mustnt', 'needn', 'neednt', 'shan', 'shant', 'shouldn', 'shouldnt', 'wasn', 'wasnt', 'weren', 'werent', 'won', 'wont', 'wouldn', 'wouldnt']\n",
    "count_vect = CountVectorizer(stop_words=stopwords)\n",
    "X = do_vectorization_transformation(count_vect, pp_reviews)\n",
    "print(\"with stopwords: \" + str(get_testing_accuracy(X, y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The testing performance with stopwords was slightly better than without, with an improvement of 0.0002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_df: 1, max_df: 1: 0.9734760802469136\n",
      "min_df: 1, max_df: 2: 0.9779128086419753\n",
      "min_df: 1, max_df: 3: 0.9740547839506173\n",
      "min_df: 1, max_df: 4: 0.9710648148148148\n",
      "min_df: 1, max_df: 5: 0.9683641975308642\n",
      "min_df: 1, max_df: 6: 0.9664351851851852\n",
      "min_df: 1, max_df: 7: 0.9653742283950617\n",
      "min_df: 1, max_df: 8: 0.9645061728395061\n",
      "min_df: 1, max_df: 9: 0.9638310185185185\n",
      "min_df: 1, max_df: 10: 0.9627700617283951\n",
      "min_df: 2, max_df: 2: 0.9616126543209876\n",
      "min_df: 2, max_df: 3: 0.9559220679012346\n",
      "min_df: 2, max_df: 4: 0.9529320987654321\n",
      "min_df: 2, max_df: 5: 0.9506172839506173\n",
      "min_df: 2, max_df: 6: 0.9488811728395061\n",
      "min_df: 2, max_df: 7: 0.9481095679012346\n",
      "min_df: 2, max_df: 8: 0.9478202160493827\n",
      "min_df: 2, max_df: 9: 0.9477237654320988\n",
      "min_df: 2, max_df: 10: 0.9478202160493827\n",
      "min_df: 3, max_df: 3: 0.9345100308641975\n",
      "min_df: 3, max_df: 4: 0.9333526234567902\n",
      "min_df: 3, max_df: 5: 0.9333526234567902\n",
      "min_df: 3, max_df: 6: 0.9333526234567902\n",
      "min_df: 3, max_df: 7: 0.9333526234567902\n",
      "min_df: 3, max_df: 8: 0.9333526234567902\n",
      "min_df: 3, max_df: 9: 0.9333526234567902\n",
      "min_df: 3, max_df: 10: 0.9333526234567902\n",
      "min_df: 4, max_df: 4: 0.9231288580246914\n",
      "min_df: 4, max_df: 5: invalid combination\n",
      "min_df: 4, max_df: 6: 0.9231288580246914\n",
      "min_df: 4, max_df: 7: 0.9231288580246914\n",
      "min_df: 4, max_df: 8: 0.9231288580246914\n",
      "min_df: 4, max_df: 9: 0.9231288580246914\n",
      "min_df: 4, max_df: 10: 0.9231288580246914\n",
      "min_df: 5, max_df: 5: 0.9135802469135802\n",
      "min_df: 5, max_df: 6: 0.9135802469135802\n",
      "min_df: 5, max_df: 7: 0.9135802469135802\n",
      "min_df: 5, max_df: 8: 0.9135802469135802\n",
      "min_df: 5, max_df: 9: 0.9135802469135802\n",
      "min_df: 5, max_df: 10: 0.9135802469135802\n",
      "min_df: 6, max_df: 6: 0.8987268518518519\n",
      "min_df: 6, max_df: 7: 0.8987268518518519\n",
      "min_df: 6, max_df: 8: 0.8987268518518519\n",
      "min_df: 6, max_df: 9: 0.8987268518518519\n",
      "min_df: 6, max_df: 10: 0.8987268518518519\n",
      "min_df: 7, max_df: 7: 0.8883101851851852\n",
      "min_df: 7, max_df: 8: 0.8883101851851852\n",
      "min_df: 7, max_df: 9: 0.8883101851851852\n",
      "min_df: 7, max_df: 10: 0.8883101851851852\n",
      "min_df: 8, max_df: 8: 0.8786651234567902\n",
      "min_df: 8, max_df: 9: 0.8786651234567902\n",
      "min_df: 8, max_df: 10: 0.8786651234567902\n",
      "min_df: 9, max_df: 9: 0.8725887345679012\n",
      "min_df: 9, max_df: 10: 0.8725887345679012\n",
      "min_df: 10, max_df: 10: 0.8668016975308642\n"
     ]
    }
   ],
   "source": [
    "# n-gram values\n",
    "for min_n in np.arange(1, 11):\n",
    "    for max_n in np.arange(min_n, 11):\n",
    "        try:\n",
    "            count_vect = CountVectorizer(ngram_range=(min_n, max_n))\n",
    "            X = do_vectorization_transformation(count_vect, pp_reviews)\n",
    "            print(\"min_df: \" + str(min_n) + \", max_df: \" + str(max_n) + \": \" + str(get_testing_accuracy(X, y)))\n",
    "        except:\n",
    "            print(\"min_df: \" + str(min_n) + \", max_df: \" + str(max_n) + \": \" + \"invalid combination\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all other factors equal, the best n-gram combination was (1, 2) with a testing accuracy of 0.9870 when using an SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing accuracy with optimal configuration with each preprocessing hyperparameter: 0.9838476729448522\n"
     ]
    }
   ],
   "source": [
    "# using the optimal configuration from all previous hyperparameter testing\n",
    "stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'youre', 'youve', 'youll', 'youd', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'shes', 'her', 'hers', 'herself', 'it', 'its', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'thatll', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'dont', 'should', 'shouldve', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'arent', 'couldn', 'couldnt', 'didn', 'didnt', 'doesn', 'doesnt', 'hadn', 'hadnt', 'hasn', 'hasnt', 'haven', 'havent', 'isn', 'isnt', 'ma', 'mightn', 'mightnt', 'mustn', 'mustnt', 'needn', 'neednt', 'shan', 'shant', 'shouldn', 'shouldnt', 'wasn', 'wasnt', 'weren', 'werent', 'won', 'wont', 'wouldn', 'wouldnt']\n",
    "count_vect = CountVectorizer(ngram_range=(1, 2),min_df=0.0, max_df=0.1, stop_words=stopwords)\n",
    "X = do_vectorization_transformation(count_vect, pp_reviews)\n",
    "print(\"testing accuracy with optimal configuration with each individual preprocessing hyperparameter: \" + str(get_testing_accuracy(X, y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training ##\n",
    "\n",
    "Will need to have consideration of how we measure performance (it likely isn't pure accuracy. Precision? Recall?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5109  200]\n",
      " [  22 5043]]\n",
      "0.9786003470213996\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "count_vect = CountVectorizer(ngram_range=(1, 2))\n",
    "X = do_vectorization_transformation(count_vect, pp_reviews)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n",
    "\n",
    "sgd_classifier = SGDClassifier(random_state=42)\n",
    "sgd_classifier.fit(X_train, y_train)\n",
    "y_pred = sgd_classifier.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(get_testing_accuracy(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.970310391363023\n",
      "2 0.970310391363023\n",
      "3 0.9568151147098516\n",
      "4 0.9568151147098516\n",
      "5 0.9552727973780606\n",
      "6 0.9556583767110083\n",
      "7 0.9550800077115866\n",
      "8 0.9539232697127434\n",
      "9 0.9527665317139001\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "X_subtrain, X_val, y_subtrain, y_val = train_test_split(X_train, y_train, test_size=0.125, random_state=123)\n",
    "for k in range(1, 10):\n",
    "    neigh = KNeighborsClassifier(n_neighbors=k)\n",
    "    neigh.fit(X_subtrain,y_subtrain)\n",
    "    y_predict = neigh.predict(X_val)\n",
    "    print(k, accuracy_score(y_val, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
